{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.6.8",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "colab": {
      "name": "lab_clfmetrics.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsharanesh/Classification-ai1-fall2019/blob/master/lab_clfmetrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jow-OLxCIre",
        "colab_type": "text"
      },
      "source": [
        "# Classification, Probabilities, and the Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3SyAq-kCIrm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "327a98ed-2c81-41ac-d5b9-1d8cab59c7c8"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib as mpl\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn.apionly as sns"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py:855: MatplotlibDeprecationWarning: \n",
            "examples.directory is deprecated; in the future, examples will be found relative to the 'datapath' directory.\n",
            "  \"found relative to the 'datapath' directory.\".format(key))\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py:846: MatplotlibDeprecationWarning: \n",
            "The text.latex.unicode rcparam was deprecated in Matplotlib 2.2 and will be removed in 3.1.\n",
            "  \"2.2\", name=key, obj_type=\"rcparam\", addendum=addendum)\n",
            "/usr/local/lib/python3.6/dist-packages/seaborn/apionly.py:9: UserWarning: As seaborn no longer sets a default style on import, the seaborn.apionly module is deprecated. It will be removed in a future version.\n",
            "  warnings.warn(msg, UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5aC1u6SCIr8",
        "colab_type": "text"
      },
      "source": [
        "We are going to encapsulate some code into handy-dandy functions that we can use for easier model training using cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZU2-loaCIsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "def cv_optimize(clf, parameters, X, y, n_jobs=1, n_folds=5, score_func=None):\n",
        "    if score_func:\n",
        "        gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds, n_jobs=n_jobs, scoring=score_func)\n",
        "    else:\n",
        "        gs = GridSearchCV(clf, param_grid=parameters, n_jobs=n_jobs, cv=n_folds)\n",
        "    gs.fit(X, y)\n",
        "    print(\"BEST\", gs.best_params_, gs.best_score_)\n",
        "    best = gs.best_estimator_\n",
        "    return best\n",
        "def do_classify(clf, parameters, indf, featurenames, targetname, target1val,mode=\"mask\", reuse_split=None, score_func=None, n_folds=5, n_jobs=1):\n",
        "    \"\"\"\n",
        "    Classification made simple (or is it more complex?)\n",
        "    THIS WORKS FOR 2 Class Classification problems only\n",
        "    parameters: parameter grid in the sklearn style\n",
        "    indf: dataframe you feed in\n",
        "    featurenames: list of columnames corresponding to features you want in your model\n",
        "    targetname: the column you want to use as target\n",
        "    target1val: the value of the \"targetname\" column\n",
        "    mode: mask or split. mask a boolean mask to choose train/test or\n",
        "        split a dictionary with keys Xtrain/Xtest/ytrain/ytest and values existing\n",
        "        training and test sets in the canonical form\n",
        "    reuse_split: the actual mask above or the actuall ditionary, depending upon which\n",
        "        modu you chose\n",
        "    score_func: this is from GridSearchCV\n",
        "    n_folds: cross val folds\n",
        "    n_jobs: mumber of processes to use in cross-validation\n",
        "    \n",
        "    We return classifier, and the train and test sets. We print accuracies\n",
        "    and the confusion matrix\n",
        "    \"\"\"\n",
        "    subdf=indf[featurenames]\n",
        "    X=subdf.values\n",
        "    y=(indf[targetname].values==target1val)*1\n",
        "    if mode==\"mask\":\n",
        "        print(\"using mask\")\n",
        "        mask=reuse_split\n",
        "        Xtrain, Xtest, ytrain, ytest = X[mask], X[~mask], y[mask], y[~mask]\n",
        "    else:\n",
        "        print(\"using reuse split\")\n",
        "        Xtrain, Xtest, ytrain, ytest = reuse_split['Xtrain'], reuse_split['Xtest'], reuse_split['ytrain'], reuse_split['ytest']\n",
        "    if parameters:\n",
        "        clf = cv_optimize(clf, parameters, Xtrain, ytrain, n_jobs=n_jobs, n_folds=n_folds, score_func=score_func)\n",
        "    clf=clf.fit(Xtrain, ytrain)\n",
        "    training_accuracy = clf.score(Xtrain, ytrain)\n",
        "    test_accuracy = clf.score(Xtest, ytest)\n",
        "    print(\"############# based on standard predict ################\")\n",
        "    print(\"Accuracy on training data: %0.2f\" % (training_accuracy))\n",
        "    print(\"Accuracy on test data:     %0.2f\" % (test_accuracy))\n",
        "    print(confusion_matrix(ytest, clf.predict(Xtest)))\n",
        "    print(\"########################################################\")\n",
        "    return clf, Xtrain, ytrain, Xtest, ytest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXYqX8AICIsR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
        "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
        "cm = plt.cm.RdBu\n",
        "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
        "\n",
        "def points_plot(ax, Xtr, Xte, ytr, yte, clf, mesh=True, colorscale=cmap_light, cdiscrete=cmap_bold, alpha=0.3, psize=10, zfunc=False):\n",
        "    h = .02\n",
        "    X=np.concatenate((Xtr, Xte))\n",
        "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
        "                         np.linspace(y_min, y_max, 100))\n",
        "\n",
        "    #plt.figure(figsize=(10,6))\n",
        "    if mesh:\n",
        "        if zfunc:\n",
        "            p0 = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 0]\n",
        "            p1 = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
        "            Z=zfunc(p0, p1)\n",
        "        else:\n",
        "            Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "        Z = Z.reshape(xx.shape)\n",
        "        plt.pcolormesh(xx, yy, Z, cmap=cmap_light, alpha=alpha, axes=ax)\n",
        "    ax.scatter(Xtr[:, 0], Xtr[:, 1], c=ytr-1, cmap=cmap_bold, s=psize, alpha=alpha,edgecolor=\"k\")\n",
        "    # and testing points\n",
        "    yact=clf.predict(Xte)\n",
        "    ax.scatter(Xte[:, 0], Xte[:, 1], c=yte-1, cmap=cmap_bold, alpha=alpha, marker=\"s\", s=psize+10)\n",
        "    ax.set_xlim(xx.min(), xx.max())\n",
        "    ax.set_ylim(yy.min(), yy.max())\n",
        "    return ax,xx,yy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS80NG7gCIsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def points_plot_prob(ax, Xtr, Xte, ytr, yte, clf, colorscale=cmap_light, cdiscrete=cmap_bold, ccolor=cm, psize=10, alpha=0.1, prob=True):\n",
        "    ax,xx,yy = points_plot(ax, Xtr, Xte, ytr, yte, clf, mesh=False, colorscale=colorscale, cdiscrete=cdiscrete, psize=psize, alpha=alpha) \n",
        "    if prob:\n",
        "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
        "    else:\n",
        "        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, cmap=ccolor, alpha=.2, axes=ax)\n",
        "    cs2 = plt.contour(xx, yy, Z, cmap=ccolor, alpha=.6, axes=ax)\n",
        "    plt.clabel(cs2, fmt = '%2.1f', colors = 'k', fontsize=14)\n",
        "    return ax "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2hLO6BrCIsm",
        "colab_type": "text"
      },
      "source": [
        "## Setting up the data\n",
        "\n",
        "(I encountered this dataset in Conway, Drew, and John White. Machine learning for hackers. \" O'Reilly Media, Inc.\", 2012.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knnqc_kuCIsp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "0dc9cbf8-9ed7-46d2-d5d0-77cae4a108cd"
      },
      "source": [
        "dfhw=pd.read_csv(\"data/01_heights_weights_genders.csv\")\n",
        "dfhw.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-29153c74c831>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfhw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/01_heights_weights_genders.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdfhw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'data/01_heights_weights_genders.csv' does not exist: b'data/01_heights_weights_genders.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Aw7KnmlCIs0",
        "colab_type": "text"
      },
      "source": [
        "We sample 500 points from 10,000, since we actually want to see trends clearly on the plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWubardOCIs4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=dfhw.sample(500, replace=False)\n",
        "np.sum(df.Gender==\"Male\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XliEHVs2CItD",
        "colab_type": "text"
      },
      "source": [
        "We split the data into training and test sets...and setup a mask so we can reuse these splits later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVNZp4ckCItM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "itrain, itest = train_test_split(range(df.shape[0]), train_size=0.6)\n",
        "mask=np.ones(df.shape[0], dtype='int')\n",
        "mask[itrain]=1#making the sample to be in training sst\n",
        "mask[itest]=0\n",
        "mask = (mask==1)#to checj which is where\n",
        "mask[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO2FFVIwCItV",
        "colab_type": "text"
      },
      "source": [
        "## Logistic regression\n",
        "\n",
        "\n",
        "$$\n",
        "\\renewcommand{\\like}{{\\cal L}}\n",
        "\\renewcommand{\\loglike}{{\\ell}}\n",
        "\\renewcommand{\\err}{{\\cal E}}\n",
        "\\renewcommand{\\dat}{{\\cal D}}\n",
        "\\renewcommand{\\hyp}{{\\cal H}}\n",
        "\\renewcommand{\\Ex}[2]{E_{#1}[#2]}\n",
        "\\renewcommand{\\x}{{\\mathbf x}}\n",
        "\\renewcommand{\\v}[1]{{\\mathbf #1}}\n",
        "$$\n",
        "\n",
        "\n",
        "Previously, we saw the loss for Logistic regression and noted that it is a loss for probability estimation...and not a loss for making decisions. We'll go into these dual losses soon..\n",
        "\n",
        "$$R_{\\cal{D}}(h(x)) = -\\loglike = -log \\like = - log(P(y|\\v{x},\\v{w})).$$\n",
        "\n",
        "\n",
        "Thus\n",
        "\n",
        "\\begin{eqnarray*}\n",
        "R_{\\cal{D}}(h(x)) &=& -log\\left(\\prod_{y_i \\in \\cal{D}} h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\right)\\\\\n",
        "                  &=& -\\sum_{y_i \\in \\cal{D}} log\\left(h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\right)\\\\                  \n",
        "                  &=& -\\sum_{y_i \\in \\cal{D}} log\\,h(\\v{w}\\cdot\\v{x_i})^{y_i} + log\\,\\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\\\\n",
        "                  &=& - \\sum_{y_i \\in \\cal{D}} \\left ( y_i log(h(\\v{w}\\cdot\\v{x})) + ( 1 - y_i) log(1 - h(\\v{w}\\cdot\\v{x})) \\right )\n",
        "\\end{eqnarray*}\n",
        "\n",
        "where\n",
        "\n",
        "$$h(z) = \\frac{1}{1 + e^{-z}}.$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ysSw5BvCItY",
        "colab_type": "text"
      },
      "source": [
        "Notice that its L2 regularized.... by default"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n5fFP9zCItd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "outputId": "f37feb4d-ae38-40fc-a799-9dd5dd93af7b"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "parameters = {\"C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}#C is the limitation constant value in the case of regularisation \n",
        "clflog = LogisticRegression(solver='lbfgs')\n",
        "clflog, Xtrain, ytrain, Xtest, ytest=do_classify(clflog, parameters, df, ['Height','Weight'],'Gender', \"Male\", mode=\"mask\", reuse_split=mask)\n",
        "#do classify is a built in function "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-2f815f6e4ce5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclflog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclflog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclflog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Height'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Gender'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Male\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PACdKZiBEi1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train=df.iloc[itrain][['Height','Weight']].values\n",
        "X_test=df.iloc[itest][['Height','Weight']].values\n",
        "y_train=1*(df.iloc[itrain]['Gender']=='Male')\n",
        "y_test=1*(df.iloc[itest\n",
        "                 \n",
        "                 ]['Gender']=='Male')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpoOy0c3EMgr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression(C=10000)\n",
        "clf.fit(X_train,y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqbABBR7CItl",
        "colab_type": "text"
      },
      "source": [
        "In `sklearn`, `clf.predict(test_data)` makes predictions on the assumption that a 0.5 probability threshold is the appropriate thing to do. Make predictions on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK26FBGGCItq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVZMDwZOCIt0",
        "colab_type": "text"
      },
      "source": [
        "In `sklearn`, `predict_proba` gives us the probabilities. Find the probabilities on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9iDyt62CIt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clflog.predict_proba(Xtest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMICDQbcCIuB",
        "colab_type": "text"
      },
      "source": [
        "What do these probabilities correspond to? The second column (`[:,1]` in numpy parlance, google numpy indexing to understand the syntax) gives the probability that the sample is a 1 (or +ive), here Male.\n",
        "\n",
        "Make a histogram of these probabilities. Interpret them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFlbXpDOCIuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD3uhmagCIuQ",
        "colab_type": "text"
      },
      "source": [
        "Lots of sure females and sure males when you plot the probability of being a male. \n",
        "\n",
        "At this point you might want to see how this histogram looks in the 2 dimensional space of our predictors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui4U9g-aCIuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Xtr=np.concatenate((Xtrain, Xtest))\n",
        "plt.figure()\n",
        "ax=plt.gca()\n",
        "with sns.plotting_context('poster'):\n",
        "    points_plot(ax, Xtrain, Xtest, ytrain, ytest, clflog);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_JxLM4vCIub",
        "colab_type": "text"
      },
      "source": [
        "We can plot the probability contours: these are rather tight!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFpLj0w7CIud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "ax=plt.gca()\n",
        "points_plot(ax, Xtrain, Xtest, ytrain, ytest, clflog, mesh=False, alpha=0.001);\n",
        "points_plot_prob(ax, Xtrain, Xtest, ytrain, ytest, clflog);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50DIhwmsCIuo",
        "colab_type": "text"
      },
      "source": [
        "The score function of the estimator is used to evaluate a parameter setting. These are the sklearn.metrics.accuracy_score for classification and sklearn.metrics.r2_score for regression. For some applications, other scoring functions are better suited (for example in unbalanced classification, the accuracy score is often uninformative). We can pass other scorers to `GridSearchCV`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2B2_FW_CIur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clflog.score(Xtest, ytest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muWL2lApCIu2",
        "colab_type": "text"
      },
      "source": [
        "## The confusion Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahi3vQHWCIu6",
        "colab_type": "text"
      },
      "source": [
        " A classifier will get some samples right, and some wrong. Generally we see which ones it gets right and which ones it gets wrong on the test set. There,\n",
        "\n",
        "- the samples that are +ive and the classifier predicts as +ive are called True Positives (TP)\n",
        "- the samples that are -ive and the classifier predicts (wrongly) as +ive are called False Positives (FP)\n",
        "- the samples that are -ive and the classifier predicts as -ive are called True Negatives (TN)\n",
        "- the samples that are +ive and the classifier predicts as -ive are called False Negatives (FN)\n",
        "\n",
        "A classifier produces a confusion matrix from these which lookslike this:\n",
        "\n",
        "![hwimages](https://github.com/rsharanesh/Classification-ai1-fall2019/blob/master/images/confusionmatrix.png?raw=1)\n",
        "\n",
        "\n",
        "IMPORTANT NOTE: In sklearn, to obtain the confusion matrix in the form above, always have the observed `y` first, i.e.: use as `confusion_matrix(y_true, y_pred)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5A3iKn5CIu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "confusion_matrix(ytest, clflog.predict(Xtest))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG4FN8LNCIvC",
        "colab_type": "text"
      },
      "source": [
        "Given these definitions, we typically calculate a few metrics for our classifier. First, the **True Positive Rate**:\n",
        "\n",
        "$$TPR = Recall = \\frac{TP}{OP} = \\frac{TP}{TP+FN},$$\n",
        "\n",
        "also called the Hit Rate: the fraction of observed positives (1s) the classifier gets right, or how many true positives were recalled. Maximizing the recall towards 1 means keeping down the false negative rate. In a classifier try to find cancer patients, this is the number we want to maximize.\n",
        "\n",
        "The **False Positive Rate** is defined as\n",
        "\n",
        "$$FPR = \\frac{FP}{ON} = \\frac{FP}{FP+TN},$$\n",
        "\n",
        "also called the False Alarm Rate, the fraction of observed negatives (0s) the classifier gets wrong. In general, you want this number to be low. Instead, you might want to maximize the\n",
        "**Precision**,which tells you how many of the predicted positive(1) hits were truly positive\n",
        "\n",
        "$$Precision = \\frac{TP}{PP} = \\frac{TP}{TP+FP}.$$\n",
        "\n",
        "Finally the **F1** score gives us the Harmonic Score of Precision and Recall. Many analysts will try and find a classifier that maximizes this score, since it tries to minimize both false positives and false negatives simultaneously, and is thus a bit more precise in what it is trying to do than the accuracy.\n",
        "\n",
        "$$F1 =  \\frac{2*Recall*Precision}{Recall + Precision}$$\n",
        "\n",
        "However, in a case like that of a cancer classifier, we will wish to minimize false nagatives at the expense of false positives: it is ok to send perfectly healthy patients for cancer folloup if that is the price we must pay for not missing any sick ones.\n",
        "\n",
        "`scikit-learn` helpfully gives us a classification report with all these numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PH1U5CUMCIvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(ytest, clflog.predict(Xtest)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VdVuhVUCIvM",
        "colab_type": "text"
      },
      "source": [
        "#### The cancer doctor\n",
        "\n",
        "Do you really want to be setting a threshold of 0.5 probability to predict if a patient has cancer or not? The false negative problem: ie the chance you predict someone dosent have cancer who has cancer is much higher for such a threshold. You could kill someone by telling them not to get a biopsy. Why not play it safe and assume a much lower threshold: for eg, if the probability of 1(cancer) is greater than 0.05, we'll call it a 1.\n",
        "\n",
        "Write a function `t_repredict(est,t, Xtest)` which takes your classifier, a probability threshold, and a  features set in the canonical form, and returns a set of predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MO5CspkVCIvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acQCno7SCIvV",
        "colab_type": "text"
      },
      "source": [
        "Print the confusion matrix to see how the false negatives get suppressed?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ok26ByqlCIvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}